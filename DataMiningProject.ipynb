{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "># Data Mining Project\n",
    "## Discover and describe aeras of interest and events from geo-located data.\n",
    "### Team:\n",
    "- **SCHLEE Adam**\n",
    "- **KUSNO Louis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">## I. Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# installation of required libraries and dependencies\n",
    "\n",
    "# numeric calculations\n",
    "! pip install numpy\n",
    "\n",
    "# data frames \n",
    "! pip install pandas\n",
    "\n",
    "# machine learning algorithms \n",
    "! pip install scikit-learn\n",
    "! pip install scipy\n",
    "\n",
    "# plotting \n",
    "! pip install matplotlib\n",
    "! pip install folium\n",
    "! pip install plotly\n",
    "! pip install plotly-express\n",
    "! pip install nbformat==5.9.2\n",
    "! pip install wordcloud\n",
    "! pip install branca\n",
    "\n",
    "# natural language processing\n",
    "! pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pandas to deal with the data\n",
    "import pandas as pd\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "# folium for maps\n",
    "import folium as fl\n",
    "# numpy\n",
    "import numpy as np\n",
    "# scaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# k-means\n",
    "from sklearn.cluster import KMeans\n",
    "# silhouette scores\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "# dendrogram\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "# agglomerative clustering\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "# DBSCAN\n",
    "from sklearn.cluster import DBSCAN\n",
    "# Nearest Neighbors\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "# Interactive plot\n",
    "import plotly.express as px \n",
    "# word cloud\n",
    "from wordcloud import WordCloud\n",
    "# stopwords and tokenization\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "# branca\n",
    "import branca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Data transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1. Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_table(\"data/flickr_data2.csv\", sep=\",\")\n",
    "\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2. Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2.2.1. Removing duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop_duplicates()\n",
    "data = data.reset_index(drop=True)\n",
    "\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2.2.2. Renaming columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = data.columns.str.replace(' ', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2.2.3. Normalizing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if \"Unnamed:16\" contains a valid year (4-digit number)\n",
    "def is_year(value):\n",
    "    return isinstance(value, (int, float)) and 1000 <= value <= 9999\n",
    "\n",
    "# Get the indexes of the rows where \"Unnamed:16\" contains a year\n",
    "index = data[data[\"Unnamed:16\"].apply(is_year)].index\n",
    "\n",
    "# Move the columns for each row concerned\n",
    "for i in index:\n",
    "    # Move the \"date_upload_*\" columns\n",
    "    data.loc[i, \"date_upload_minute\"] = data.loc[i, \"date_upload_hour\"]\n",
    "    data.loc[i, \"date_upload_hour\"] = data.loc[i, \"date_upload_day\"]\n",
    "    data.loc[i, \"date_upload_day\"] = data.loc[i, \"date_upload_month\"]\n",
    "    data.loc[i, \"date_upload_month\"] = data.loc[i, \"date_upload_year\"]\n",
    "    data.loc[i, \"date_upload_year\"] = data.loc[i, \"Unnamed:16\"]\n",
    "    data.loc[i, \"Unnamed:16\"] = None\n",
    "    \n",
    "    # Move the \"date_taken_*\" columns\n",
    "    if \"date_taken_minute\" in data.columns:\n",
    "        tmpYear = data.loc[i, \"date_taken_minute\"]\n",
    "        data.loc[i, \"date_taken_minute\"] = data.loc[i, \"date_taken_hour\"]\n",
    "        data.loc[i, \"date_taken_hour\"] = data.loc[i, \"date_taken_day\"]\n",
    "        data.loc[i, \"date_taken_day\"] = data.loc[i, \"date_taken_month\"]\n",
    "        data.loc[i, \"date_taken_month\"] = data.loc[i, \"date_taken_year\"]\n",
    "        data.loc[i, \"date_taken_year\"] = tmpYear\n",
    "\n",
    "# Delete the \"Unnamed\" columns\n",
    "data = data.drop(columns=data.columns[data.columns.str.contains(\"Unnamed\")])\n",
    "\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2.2.4. Change data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"date_upload_minute\"].unique()\n",
    "data[\"date_upload_minute\"] = pd.to_numeric(data[\"date_upload_minute\"], errors=\"coerce\")\n",
    "data[\"date_upload_minute\"] = data[\"date_upload_minute\"].astype(\"Int64\")\n",
    "data[\"date_upload_year\"] = data[\"date_upload_year\"].astype(\"Int64\")\n",
    "\n",
    "# We remove duplicates that may have been revealed after the modifications\n",
    "\n",
    "data = data.drop_duplicates()\n",
    "data = data.reset_index(drop=True)\n",
    "    \n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Saving cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"data/dataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks to the data cleaning, we will be able to work on a clean dataset.\n",
    "The original dataset had 420241 entries. \n",
    "After cleaning, we have 168099 entries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Data Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Define the number of samples\n",
    "n_samples = 50000 #168098 # Maximum number of samples\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv(\"data/dataset.csv\")\n",
    "\n",
    "# Sample the data\n",
    "data = data.sample(n_samples)\n",
    "\n",
    "# Reset the index\n",
    "data = data.reset_index(drop=True)\n",
    "\n",
    "# Display the data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the data on a map using Folium\n",
    "\n",
    "# List of colors\n",
    "colors = [\n",
    "    \"red\", \"blue\", \"green\", \"yellow\", \"purple\", \"orange\", \"pink\", \"white\", \"brown\", \"cyan\", \"magenta\", \"olive\", \"lime\", \"navy\", \"teal\", \"maroon\", \"silver\", \"gold\", \"crimson\"\n",
    "]\n",
    "\n",
    "# Create the map\n",
    "map = fl.Map(location=[48.8566, 2.3522], zoom_start=12)\n",
    "\n",
    "# Add a layer to the map\n",
    "fl.TileLayer('Cartodb dark_matter').add_to(map)\n",
    "\n",
    "# Display the data on the map\n",
    "for i in range(0, len(data)):\n",
    "    fl.Circle(\n",
    "        location=[data.loc[i]['lat'], data.loc[i]['long']],\n",
    "        radius=10,\n",
    "        color='crimson',\n",
    "        fill=True,\n",
    "        fill_color='dark'\n",
    "    ).add_to(map)\n",
    "\n",
    "# Fit the map to the bounds\n",
    "map.fit_bounds(map.get_bounds())\n",
    "\n",
    "# Display the map\n",
    "map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Data Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1. K-means clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.3.1.1. Scaling data for K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns to keep\n",
    "keep_col = ['lat', 'long']\n",
    "df_clustering = data[keep_col]\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(df_clustering)\n",
    "scaled_data_df = pd.DataFrame(data=scaled_data, columns=df_clustering.columns)\n",
    "scaled_data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.3.1.2. Looking for the optimal number of clusters with the Elbow method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Range of k (number of clusters)\n",
    "range_k = range(1, len(scaled_data_df)//1000)\n",
    "\n",
    "# List to store the inertia values (sum of squared distances)\n",
    "inertias = []\n",
    "\n",
    "# Loop over the range of k values to apply k-means\n",
    "for i in range_k:\n",
    "    # Apply k-means with i clusters\n",
    "    print(f\"Applying k-means with {i}/{len(scaled_data_df)//1000} clusters\")\n",
    "    kmeans = KMeans(n_clusters=i, init='k-means++')\n",
    "    # Fit the data to the model\n",
    "    kmeans.fit(scaled_data_df)\n",
    "    # Append the inertia value to the list\n",
    "    inertias.append(kmeans.inertia_)\n",
    "\n",
    "# Visualize the inertia values to find the optimal number of clusters\n",
    "n = len(inertias)//4\n",
    "xticks_new = np.arange(1, n+1)\n",
    "plt.plot(xticks_new, inertias[0:n], 'bx-')\n",
    "plt.title('Finding the optimal number of clusters')\n",
    "plt.xticks(xticks_new)\n",
    "plt.xlabel('# clusters')\n",
    "plt.ylabel('Sum of squared distances')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Elbow method, we found that the optimal number of clusters is very small (around 3, 4, 5 or 6 clusters)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.3.1.3. Applying K-means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of clusters (found by the elbow method)\n",
    "k = 6\n",
    "\n",
    "# A bigger number of clusters\n",
    "k_big = 100\n",
    "\n",
    "# Create the k-means model\n",
    "kmeans = KMeans(n_clusters=k, init='k-means++')\n",
    "kmeans_big = KMeans(n_clusters=k_big, init='k-means++')\n",
    "\n",
    "# Fit the data to the model\n",
    "kmeans.fit(scaled_data_df)\n",
    "kmeans_big.fit(scaled_data_df)\n",
    "\n",
    "# Predict the clusters\n",
    "labels = kmeans.labels_\n",
    "print(f\"k-means labels: {labels}\")\n",
    "\n",
    "labels_big = kmeans_big.labels_\n",
    "print(f\"k-means labels (big): {labels_big}\")\n",
    "\n",
    "inertia = kmeans.inertia_\n",
    "print(f\"Sum of squared distances: {inertia}\")\n",
    "\n",
    "inertia_big = kmeans_big.inertia_\n",
    "print(f\"Sum of squared distances (big): {inertia_big}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.3.1.4. Visualizing K-means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the clusters on a map using Folium with different colors for each cluster\n",
    "\n",
    "# Add the cluster labels to the data\n",
    "data['cluster'] = labels\n",
    "data['cluster_big'] = labels_big\n",
    "\n",
    "# Create the maps\n",
    "map = fl.Map(\n",
    "    location=[48.8566, 2.3522], \n",
    "    zoom_start=12,\n",
    "    position='absolute',\n",
    "    left='0%',\n",
    "    width='50%',\n",
    "    height='100%'\n",
    ")\n",
    "\n",
    "map_big = fl.Map(\n",
    "    location=[48.8566, 2.3522],\n",
    "    zoom_start=12,\n",
    "    position='absolute',\n",
    "    left='50%',\n",
    "    width='50%',\n",
    "    height='100%'\n",
    ")\n",
    "\n",
    "# Add a layer to the map\n",
    "fl.TileLayer('Cartodb dark_matter').add_to(map)\n",
    "fl.TileLayer('Cartodb dark_matter').add_to(map_big)\n",
    "\n",
    "for i in range(0, len(data)):\n",
    "    fl.Circle(\n",
    "        location=[data.loc[i]['lat'], data.loc[i]['long']],\n",
    "        radius=2,\n",
    "        color=colors[data.loc[i]['cluster']%len(colors)],\n",
    "        fill=True,\n",
    "        fill_color=colors[data.loc[i]['cluster']%len(colors)],\n",
    "        popup=f\"Cluster: {data.loc[i]['cluster']}\"\n",
    "    ).add_to(map)\n",
    "\n",
    "    fl.Circle(\n",
    "        location=[data.loc[i]['lat'], data.loc[i]['long']],\n",
    "        radius=2,\n",
    "        color=colors[data.loc[i]['cluster_big']%len(colors)],\n",
    "        fill=True,\n",
    "        fill_color=colors[data.loc[i]['cluster_big']%len(colors)],\n",
    "        popup=f\"Cluster: {data.loc[i]['cluster_big']}\"\n",
    "    ).add_to(map_big)\n",
    "\n",
    "# Fit the maps to the bounds\n",
    "map.fit_bounds(map.get_bounds())\n",
    "map_big.fit_bounds(map_big.get_bounds())  \n",
    "\n",
    "# Display the maps in a subplot\n",
    "f = branca.element.Figure()\n",
    "f.add_child(map)\n",
    "f.add_child(map_big)\n",
    "f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.3.1.5. Silhouette score for K-means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette_avg = silhouette_score(scaled_data, labels, metric='euclidean')\n",
    "sample_silhouette_values = silhouette_samples(scaled_data, labels, metric='euclidean')\n",
    "data['silhouette kmeans'] = sample_silhouette_values\n",
    "\n",
    "silhouette_avg_big = silhouette_score(scaled_data, labels_big, metric='euclidean')\n",
    "sample_silhouette_values_big = silhouette_samples(scaled_data, labels_big, metric='euclidean')\n",
    "data['silhouette kmeans big'] = sample_silhouette_values_big\n",
    "\n",
    "print(f\"Average silhouette score: {silhouette_avg}\")\n",
    "print(f\"Sample Silhouette values: {sample_silhouette_values}\")\n",
    "\n",
    "print(f\"Average silhouette score (big): {silhouette_avg_big}\")\n",
    "print(f\"Sample Silhouette values (big): {sample_silhouette_values_big}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Silhouette plot for k-means\n",
    "\n",
    "y_lower = 10\n",
    "\n",
    "for i in range(k):\n",
    "    # Get silhouette scores for cluster i\n",
    "    ith_cluster_values = sample_silhouette_values[labels == i]\n",
    "    ith_cluster_values.sort()\n",
    "    \n",
    "    size_cluster_i = ith_cluster_values.shape[0]\n",
    "    y_upper = y_lower + size_cluster_i\n",
    "    \n",
    "    # Fill the silhouette\n",
    "    ax1.fill_betweenx(np.arange(y_lower, y_upper), 0, ith_cluster_values, alpha=0.7)\n",
    "    \n",
    "    # Label the silhouette plots\n",
    "    ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, f'Cluster {i}')\n",
    "        \n",
    "    y_lower = y_upper + 10\n",
    "\n",
    "# Silhouette plot for k-means (big)\n",
    "\n",
    "y_lower = 10\n",
    "\n",
    "for i in range(k_big):\n",
    "    # Get silhouette scores for cluster i\n",
    "    ith_cluster_values = sample_silhouette_values_big[labels_big == i]\n",
    "    ith_cluster_values.sort()\n",
    "    \n",
    "    size_cluster_i = ith_cluster_values.shape[0]\n",
    "    y_upper = y_lower + size_cluster_i\n",
    "    \n",
    "    # Fill the silhouette\n",
    "    ax2.fill_betweenx(np.arange(y_lower, y_upper), 0, ith_cluster_values, alpha=0.7)\n",
    "    \n",
    "    # Label the silhouette plots\n",
    "    ax2.text(-0.05, y_lower + 0.5 * size_cluster_i, f'Cluster {i}')\n",
    "\n",
    "    y_lower = y_upper + 10\n",
    "\n",
    "# Add vertical line for average silhouette score\n",
    "ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "ax1.set_yticks([])\n",
    "ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "ax1.set_xlabel(f\"Silhouette coefficient values for k = {k}\")\n",
    "ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "# Add vertical line for average silhouette score (big)\n",
    "ax2.axvline(x=silhouette_avg_big, color=\"red\", linestyle=\"--\")\n",
    "ax2.set_yticks([])\n",
    "ax2.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "ax2.set_xlabel(f\"Silhouette coefficient values for k = {k_big}\")\n",
    "ax2.set_ylabel(\"Cluster label\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2. Hierarchical clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.3.2.1. Function definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot the dendrogram of the hierarchical clustering\n",
    "def plot_dendrogram(model, lbls, title='Hierarchical Clustering Dendrogram', x_title='Clusters', **kwargs):\n",
    "    # Create linkage matrix and then plot the dendrogram\n",
    "\n",
    "    # Create the counts of samples under each node\n",
    "    counts = np.zeros(model.children_.shape[0])\n",
    "    n_samples = len(model.labels_)\n",
    "    for i, merge in enumerate(model.children_):\n",
    "        current_count = 0\n",
    "        for child_idx in merge:\n",
    "            if child_idx < n_samples:\n",
    "                current_count += 1\n",
    "            else:\n",
    "                current_count += counts[child_idx - n_samples]\n",
    "        counts[i] = current_count\n",
    "\n",
    "    linkage_matrix = np.column_stack([\n",
    "           model.children_,\n",
    "           model.distances_,\n",
    "           counts\n",
    "       ]).astype(float)\n",
    "\n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Plot the corresponding dendrogram\n",
    "    dendrogram(linkage_matrix, labels=lbls, leaf_rotation=90)\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.xlabel(x_title)\n",
    "    plt.ylabel('Distance')\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "\n",
    "# Hierarchical clustering with different linkages\n",
    "def hierarchical(data, labels, metric='euclidean', linkage='average', n_clusters=None, dist_thres=None):\n",
    "    model = AgglomerativeClustering(distance_threshold=dist_thres, n_clusters=n_clusters, metric=metric, linkage=linkage, compute_full_tree=True, compute_distances=True)\n",
    "    model = model.fit(data)\n",
    "    \n",
    "    txt_title = 'Hierarchical Clustering Dendrogram' + ', linkage: ' + linkage + ', n_clusters: ' + str(n_clusters)\n",
    "    f = plot_dendrogram(model=model, lbls=labels, title=txt_title, x_title='Clusters')\n",
    "    \n",
    "    return model, f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.3.2.2. Applying hierarchical clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample the data\n",
    "hierarchical_data = data.sample(1000)\n",
    "hierarchical_data = hierarchical_data.reset_index(drop=True)\n",
    "\n",
    "# Select columns to keep\n",
    "keep_col = ['lat', 'long']\n",
    "df_clustering = hierarchical_data[keep_col]\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "scaled_data_hierarchical = scaler.fit_transform(df_clustering)\n",
    "scaled_data_df_hierarchical = pd.DataFrame(data=scaled_data_hierarchical, columns=df_clustering.columns)\n",
    "\n",
    "# Number of clusters\n",
    "k_hierarchical = 50\n",
    "\n",
    "linkage = ['complete', 'average', 'single']\n",
    "\n",
    "for link in linkage:\n",
    "    m, f = hierarchical(scaled_data_hierarchical, list(scaled_data_df_hierarchical.index), metric='euclidean', linkage=link, n_clusters=k_hierarchical, dist_thres=None)\n",
    "    \n",
    "    hierarchical_data['cluster ' + link] = m.labels_\n",
    "    silhouette_avg = silhouette_score(scaled_data_hierarchical, m.labels_, metric='euclidean')\n",
    "    sample_silhouette_values = silhouette_samples(scaled_data_hierarchical, m.labels_, metric='euclidean')\n",
    "    hierarchical_data['silhouette ' + link] = sample_silhouette_values\n",
    "    \n",
    "    print(f\"Linkage: {link}, silhouette score: {silhouette_avg}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.3.2.3. Visualizing hierarchical clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the maps\n",
    "map_single = fl.Map(\n",
    "    location=[48.8566, 2.3522], \n",
    "    zoom_start=12,\n",
    "    position='absolute',\n",
    "    left='0%',\n",
    "    width='33%',\n",
    "    height='100%'\n",
    ")\n",
    "\n",
    "map_complete = fl.Map(\n",
    "    location=[48.8566, 2.3522],\n",
    "    zoom_start=12,\n",
    "    position='absolute',\n",
    "    left='33%',\n",
    "    width='33%',\n",
    "    height='100%'\n",
    ")\n",
    "\n",
    "map_average = fl.Map(\n",
    "    location=[48.8566, 2.3522],\n",
    "    zoom_start=12,\n",
    "    position='absolute',\n",
    "    left='66%',\n",
    "    width='33%',\n",
    "    height='100%'\n",
    ")\n",
    "\n",
    "# Add a layer to the maps\n",
    "fl.TileLayer('Cartodb dark_matter').add_to(map_single)\n",
    "fl.TileLayer('Cartodb dark_matter').add_to(map_complete)\n",
    "fl.TileLayer('Cartodb dark_matter').add_to(map_average)\n",
    "\n",
    "# add points to the map with different colors for each cluster\n",
    "for i in range(0, len(hierarchical_data)):\n",
    "\n",
    "    fl.Circle(\n",
    "        location=[hierarchical_data.loc[i]['lat'], hierarchical_data.loc[i]['long']],\n",
    "        radius=2,\n",
    "        color=colors[hierarchical_data.loc[i]['cluster single']%len(colors)],\n",
    "        fill=True,\n",
    "        fill_color=colors[hierarchical_data.loc[i]['cluster single']%len(colors)],\n",
    "        popup=f\"Cluster: {hierarchical_data.loc[i]['cluster single']}\"\n",
    "    ).add_to(map_single)\n",
    "\n",
    "    fl.Circle(\n",
    "        location=[hierarchical_data.loc[i]['lat'], hierarchical_data.loc[i]['long']],\n",
    "        radius=2,\n",
    "        color=colors[hierarchical_data.loc[i]['cluster average']%len(colors)],\n",
    "        fill=True,\n",
    "        fill_color=colors[hierarchical_data.loc[i]['cluster average']%len(colors)],\n",
    "        popup=f\"Cluster: {hierarchical_data.loc[i]['cluster average']}\"\n",
    "    ).add_to(map_average)\n",
    "\n",
    "    fl.Circle(\n",
    "        location=[hierarchical_data.loc[i]['lat'], hierarchical_data.loc[i]['long']],\n",
    "        radius=2,\n",
    "        color=colors[hierarchical_data.loc[i]['cluster complete']%len(colors)],\n",
    "        fill=True,\n",
    "        fill_color=colors[hierarchical_data.loc[i]['cluster complete']%len(colors)],\n",
    "        popup=f\"Cluster: {hierarchical_data.loc[i]['cluster complete']}\"\n",
    "    ).add_to(map_complete)\n",
    "\n",
    "\n",
    "# Fit the maps to the bounds\n",
    "map_single.fit_bounds(map_single.get_bounds())\n",
    "map_complete.fit_bounds(map_complete.get_bounds())\n",
    "map_average.fit_bounds(map_average.get_bounds())\n",
    "\n",
    "# Display the maps in a subplot\n",
    "f = branca.element.Figure()\n",
    "f.add_child(map_single)\n",
    "f.add_child(map_average)\n",
    "f.add_child(map_complete)\n",
    "f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.3. DBSCAN clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.3.3.1. Looking for the optimal epsilon value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimum number of points in a neighborhood to define a cluster\n",
    "min_pts = len(data)//1000\n",
    "\n",
    "def find_optimal_eps(data, min_pts):\n",
    "    # Calculate distances to k-nearest neighbors\n",
    "    neigh = NearestNeighbors(n_neighbors=min_pts)\n",
    "    neigh.fit(data)\n",
    "    distances, _ = neigh.kneighbors(data)\n",
    "    \n",
    "    \n",
    "    # Sort distances to kth neighbor in ascending order\n",
    "    k_distances = np.sort(distances[:, min_pts-1])\n",
    "    \n",
    "    # Create plot\n",
    "    fig = px.line(\n",
    "        x=range(len(k_distances)),\n",
    "        y=k_distances,\n",
    "        title=f'K-distance Graph (k={min_pts})',\n",
    "        labels={'x': 'Points sorted by distance', \n",
    "                'y': f'Distance to {min_pts}th nearest neighbor'}\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "fig = find_optimal_eps(scaled_data, min_pts)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method is not suitable for our dataset because it is too large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.3.3.2. Selecting the optimal epsilon value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to our tests, the optimal epsilon value that we found is $0.004$.\n",
    "With a min_samples value of $len(data) // 1000$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_eps = 0.006\n",
    "min_samples = len(data)//1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.3.3.3. Applying DBSCAN clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan = DBSCAN(eps=optimal_eps, min_samples=min_samples)\n",
    "labels = dbscan.fit_predict(scaled_data)\n",
    "\n",
    "# Print number of clusters and noise points\n",
    "n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_noise = list(labels).count(-1)\n",
    "print(f\"\\nNumber of clusters: {n_clusters}\")\n",
    "print(f\"Number of noise points: {n_noise}\")\n",
    "\n",
    "data['cluster dbscan'] = labels\n",
    "\n",
    "# Remove line that have no cluster (cluster dbscan == -1)\n",
    "no_noise_data = data[data['cluster dbscan'] != -1]\n",
    "no_noise_data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.3.3.4. Visualizing DBSCAN clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the map\n",
    "map = fl.Map(location=[48.8566, 2.3522], zoom_start=12)\n",
    "\n",
    "# Add a layer to the map\n",
    "fl.TileLayer('Cartodb dark_matter').add_to(map)\n",
    "\n",
    "# Add points to the map with different colors for each cluster\n",
    "for i in range(len(no_noise_data)):  # Utiliser la taille réelle des données\n",
    "    cluster = no_noise_data.loc[i]['cluster dbscan']\n",
    "    \n",
    "    # Ignore noise points (-1)\n",
    "    if cluster == -1:\n",
    "        continue\n",
    "    \n",
    "    # Add cluster points to the map\n",
    "    fl.Circle(\n",
    "        location=[no_noise_data.loc[i]['lat'], no_noise_data.loc[i]['long']],\n",
    "        radius=2,\n",
    "        color=colors[cluster % len(colors)],\n",
    "        fill=True,\n",
    "        fill_color=colors[cluster % len(colors)],\n",
    "        popup=f\"Cluster: {cluster}\\n\"\n",
    "    ).add_to(map)\n",
    "\n",
    "# Fit the map to the bounds\n",
    "map.fit_bounds(map.get_bounds())\n",
    "\n",
    "# Show the map\n",
    "map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.3.3.5. Silhouette score for DBSCAN clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale no noise data\n",
    "keep_col = ['lat', 'long']\n",
    "no_noise_df_clustering = no_noise_data[keep_col]\n",
    "no_noise_scaler = StandardScaler()\n",
    "no_noise_scaled_data = scaler.fit_transform(no_noise_df_clustering)\n",
    "no_noise_scaled_data_df = pd.DataFrame(data=no_noise_scaled_data, columns=no_noise_df_clustering.columns)\n",
    "\n",
    "# Redefine labels\n",
    "labels = no_noise_data['cluster dbscan']\n",
    "\n",
    "# Silhouette score\n",
    "silhouette_avg = silhouette_score(no_noise_scaled_data, labels, metric='euclidean')\n",
    "sample_silhouette_values = silhouette_samples(no_noise_scaled_data, labels, metric='euclidean')\n",
    "no_noise_data['silhouette dbscan'] = sample_silhouette_values\n",
    "\n",
    "print(f\"Average silhouette score: {silhouette_avg}\")\n",
    "print(f\"Sample Silhouette values: {sample_silhouette_values}\")\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(15, 5))\n",
    "\n",
    "# Silhouette plot for DBSCAN\n",
    "\n",
    "y_lower = 10\n",
    "\n",
    "for i in range(n_clusters):\n",
    "    # Get silhouette scores for cluster i\n",
    "    ith_cluster_values = sample_silhouette_values[labels == i]\n",
    "    ith_cluster_values.sort()\n",
    "    \n",
    "    size_cluster_i = ith_cluster_values.shape[0]\n",
    "    y_upper = y_lower + size_cluster_i\n",
    "    \n",
    "    # Fill the silhouette\n",
    "    ax.fill_betweenx(np.arange(y_lower, y_upper), 0, ith_cluster_values, alpha=0.7)\n",
    "    \n",
    "    # Label the silhouette plots\n",
    "    ax.text(-0.05, y_lower + 0.5 * size_cluster_i, f'Cluster {i}')\n",
    "        \n",
    "    y_lower = y_upper + 10\n",
    "\n",
    "# Add vertical line for average silhouette score\n",
    "ax.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "ax.set_yticks([])\n",
    "ax.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "ax.set_xlabel(f\"Silhouette coefficient values for DBSCAN (eps={optimal_eps}, min_samples={min_samples}) with average silhouette score: {silhouette_avg}\")\n",
    "ax.set_ylabel(\"Cluster label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">## III. Text pattern mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with other types of data, data preprocessing plays a major role with textual data.\n",
    "- Removing stopwords (words that are used a lot while not bringing meaningful information,\n",
    "such as “is”, “the”, “a”, ... or their french equivalent “est”, “le”, “un”, ...).\n",
    "- Similarly, it will be interesting to remove frequent words in the dataset that are not a\n",
    "stopword or meaningful (e.g. “picture”). You might consider visualizing the data with a\n",
    "word cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning of tags and title columns\n",
    "\n",
    "# We remove special characters and numbers in tags and title, but not accents or other common characters\n",
    "no_noise_data['tags'] = no_noise_data['tags'].str.replace(r'[^a-zA-Z éèôàëêöâäîïùûüç]', ' ', regex=True)\n",
    "no_noise_data['title'] = no_noise_data['title'].str.replace(r'[^a-zA-Z éèôàëêöâäîïùûüç]', ' ', regex=True)\n",
    "\n",
    "# We put everything in lowercase\n",
    "no_noise_data['tags'] = no_noise_data['tags'].str.lower()\n",
    "no_noise_data['title'] = no_noise_data['title'].str.lower()\n",
    "\n",
    "# Replace NaN with empty strings\n",
    "no_noise_data['tags'].fillna('', inplace=True)\n",
    "no_noise_data['title'].fillna('', inplace=True)\n",
    "\n",
    "# Delete English and French stopwords in title\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words_fr = set(stopwords.words('french'))\n",
    "no_noise_data['title'] = no_noise_data['title'].apply(lambda x: ' '.join([item for item in x.split() if item not in stop_words]))\n",
    "no_noise_data['title'] = no_noise_data['title'].apply(lambda x: ' '.join([item for item in x.split() if item not in stop_words_fr]))\n",
    "\n",
    "# Delete frequent words and solo letters\n",
    "frequent_words = ['photo', 'ddc', 'groupeserveur', 'picture', 'lumix', 'panasonic', 'image', 'photography', 'photograph', 'photographie', 'instagram', 'instagramapp', 'uploaded', 'lyon', 'france', 'flickr', 'photographer', 'photographie', 'streetphotography','iphone','lesphotosdevoyage','img','jpg','jpeg','png','iphoneography']\n",
    "solo_letters = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
    "no_noise_data['title'] = no_noise_data['title'].apply(lambda x: ' '.join([item for item in x.split() if item not in frequent_words and item not in solo_letters]))\n",
    "no_noise_data['tags'] = no_noise_data['tags'].apply(lambda x: ' '.join([item for item in x.split() if item not in frequent_words and item not in solo_letters]))\n",
    "\n",
    "# Delete words with less than 3 characters\n",
    "no_noise_data['title'] = no_noise_data['title'].apply(lambda x: ' '.join([item for item in x.split() if len(item) > 3]))\n",
    "no_noise_data['tags'] = no_noise_data['tags'].apply(lambda x: ' '.join([item for item in x.split() if len(item) > 3]))\n",
    "\n",
    "# Delete words that contain frequent words\n",
    "no_noise_data['title'] = no_noise_data['title'].apply(lambda x: ' '.join([item for item in x.split() if not any(word in item for word in frequent_words)]))\n",
    "no_noise_data['tags'] = no_noise_data['tags'].apply(lambda x: ' '.join([item for item in x.split() if not any(word in item for word in frequent_words)]))\n",
    "\n",
    "# We clean the no_noise_data again\n",
    "no_noise_data['tags'].fillna('', inplace=True)\n",
    "no_noise_data['title'].fillna('', inplace=True)\n",
    "\n",
    "# Join tags and titles in a new column\n",
    "no_noise_data['text'] = no_noise_data['title'] + ' ' + no_noise_data['tags']\n",
    "                                    \n",
    "# We try to tokenize the text column, but tokenization is ineffective for tags and title\n",
    "# because words are often stuck together, and there is no punctuation, verbs, etc.\n",
    "# Tokenization is therefore useless for this particular dataset\n",
    "# without using word_tokenize\n",
    "no_noise_data['text'] = no_noise_data['text'].apply(lambda x: word_tokenize(x))\n",
    "\n",
    "# We create a list of all the words in the text column\n",
    "words = []\n",
    "for i in range(len(no_noise_data)):\n",
    "    words += no_noise_data.loc[i]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Text vizualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a word cloud with the words in the text column\n",
    "wordcloud = WordCloud(width = 800, height = 800,\n",
    "            background_color ='white',\n",
    "            min_font_size = 10).generate(' '.join(words))\n",
    "\n",
    "plt.figure(figsize = (8, 8), facecolor = None)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad = 0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Text clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the words and their frequency in a dictionary\n",
    "words = {}\n",
    "for i in range(len(no_noise_data)):\n",
    "    for word in no_noise_data.at[i, 'text']:\n",
    "        if word in words:\n",
    "            words[word] += 1\n",
    "        else:\n",
    "            words[word] = 1\n",
    "\n",
    "# We create a dictionary with the words and their frequency by cluster\n",
    "words_by_cluster = {}\n",
    "for i in range(n_clusters):\n",
    "    words_by_cluster[i] = {}\n",
    "\n",
    "for i in range(len(no_noise_data)):\n",
    "    for word in no_noise_data.at[i, 'text']:\n",
    "        if word in words_by_cluster[no_noise_data.at[i, 'cluster dbscan']]:\n",
    "            words_by_cluster[no_noise_data.at[i, 'cluster dbscan']][word] += 1\n",
    "        else:\n",
    "            words_by_cluster[no_noise_data.at[i, 'cluster dbscan']][word] = 1\n",
    "\n",
    "# Calculating TF-IDF for each word (Term Frequency - Inverse Document Frequency)\n",
    "tf_idf = {}\n",
    "\n",
    "for i in range(n_clusters):\n",
    "    tf_idf[i] = {}\n",
    "\n",
    "for i in range(len(no_noise_data)):\n",
    "    for word in no_noise_data.at[i, 'text']:\n",
    "        tf = words_by_cluster[no_noise_data.at[i, 'cluster dbscan']][word] / len(no_noise_data)\n",
    "        idf = len(no_noise_data) / words[word]\n",
    "        tf_idf[no_noise_data.at[i, 'cluster dbscan']][word] = tf * idf\n",
    "    no_noise_data.at[i, 'text'] = ' '.join(no_noise_data.at[i, 'text'])\n",
    "\n",
    "# We add a column for each line with the 10 most important words of the cluster to which it belongs\n",
    "for i in range(len(no_noise_data)):\n",
    "    no_noise_data.at[i, 'important_words'] = ' '.join(sorted(tf_idf[no_noise_data.at[i, 'cluster dbscan']], key=tf_idf[no_noise_data.at[i, 'cluster dbscan']].get, reverse=True)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Date classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column date to datetime format and creation of a column date with the date and time of the photo taken in datetime format (year, month, day, hour, minute)\n",
    "no_noise_data.rename(columns={'date_taken_year': 'year', 'date_taken_month': 'month', 'date_taken_day': 'day', 'date_taken_hour': 'hour', 'date_taken_minute': 'minute'}, inplace=True)\n",
    "no_noise_data['date'] = pd.to_datetime(no_noise_data[['year', 'month', 'day', 'hour', 'minute']])\n",
    "\n",
    "# We create a dictionary with the min and max dates for each cluster\n",
    "dates = {}\n",
    "for i in range(n_clusters):\n",
    "    dates[i] = {'min': None, 'max': None}\n",
    "for i in range(len(no_noise_data)):\n",
    "    if dates[no_noise_data.at[i, 'cluster dbscan']]['min'] is None or no_noise_data.at[i, 'date'] < dates[no_noise_data.at[i, 'cluster dbscan']]['min']:\n",
    "        dates[no_noise_data.at[i, 'cluster dbscan']]['min'] = no_noise_data.at[i, 'date']\n",
    "    if dates[no_noise_data.at[i, 'cluster dbscan']]['max'] is None or no_noise_data.at[i, 'date'] > dates[no_noise_data.at[i, 'cluster dbscan']]['max']:\n",
    "        dates[no_noise_data.at[i, 'cluster dbscan']]['max'] = no_noise_data.at[i, 'date']\n",
    "\n",
    "# We now look at the difference between the min and max dates for each cluster\n",
    "# If the difference is very large (arbitrarily more than 1 month), this means that it is a permanent temporal cluster\n",
    "# We therefore add another column for each cluster that indicates whether it is a punctual or permanent temporal cluster\n",
    "\n",
    "for i in range(len(no_noise_data)):\n",
    "    if (dates[no_noise_data.at[i, 'cluster dbscan']]['max'] - dates[no_noise_data.at[i, 'cluster dbscan']]['min']).days > 30:\n",
    "        no_noise_data.at[i, 'temporal'] = 'permanent'\n",
    "    else:\n",
    "        no_noise_data.at[i, 'temporal'] = 'punctual'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5. Final Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map = fl.Map(location=[48.8566, 2.3522], zoom_start=12)\n",
    "\n",
    "fl.TileLayer('Cartodb dark_matter').add_to(map)\n",
    "\n",
    "# Add points to the map with different colors for each cluster\n",
    "for i in range(len(no_noise_data)):\n",
    "\n",
    "    cluster = no_noise_data.loc[i]['cluster dbscan']\n",
    "    temporal_cluster = no_noise_data.loc[i]['temporal_cluster']\n",
    "    important_words = no_noise_data.loc[i]['important_words']\n",
    "    \n",
    "    fl.Circle(\n",
    "        location=[no_noise_data.at[i,\"lat\"], no_noise_data.at[i,\"long\"]], \n",
    "        popup=f\"Cluster: {cluster}\\n{temporal_cluster}\",\n",
    "        tooltip=f\"{important_words}\",\n",
    "        radius = 10,\n",
    "        color = colors[cluster % len(colors)],\n",
    "        fill_color = colors[cluster % len(colors)],\n",
    "        fill = True,\n",
    "    ).add_to(map)\n",
    "\n",
    "# Fit the map to the bounds\n",
    "map.fit_bounds(map.get_bounds())\n",
    "\n",
    "# Display the map\n",
    "map.save(\"map.html\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
